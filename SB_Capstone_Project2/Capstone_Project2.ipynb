{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"H\"> Sentiment Analysis on Movie Reviews</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href=\"#C1\">1. Problem Description</a>\n",
    "- <a href=\"#C2\">2. Load the Dataset and Have a First Look</a>\n",
    "- <a href=\"#C3\">3. Preprocessing of Text: Vectorizer</a>\n",
    "- <a href=\"#C4\">4. Modeling, Evaluation and Improving the Model</a>\n",
    "- <a href=\"#C5\">5. Predict the Sentiment labels of Sentiment_unknow Set</a>\n",
    "- <a href=\"#C6\">6. Conclusion</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"C1\">1. Problem Description</h2>\n",
    "\n",
    "<p>\n",
    "Opinion mining or sentiment analysis aims to determine the attitude of a critic or customer or other subject with respected to a topic. And in this project, attitude of critics and users towards various movies is what we are concerned. \n",
    "</p>\n",
    "<p>Cognitive insight is important for online streaming or shopping website, because it can help to predict whether customers like or dislike a movie or other products. When they submit a new comment, the website can offer some feedback according to customersâ€™ attitude. And in this project, the data is from kaggle.com.\n",
    "</p>\n",
    "<p>And the goal of the project is to produce an algorithm to classify phrases into 5 categories: negative, somewhat negative, neutral, somewaht postive and positive.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "senti_known= pd.read_csv('train.tsv/train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "senti_unknown = pd.read_csv('test.tsv/test.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id='C2'>2. Load the Dataset and Have a First Look</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data set with sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      "PhraseId      156060 non-null int64\n",
      "SentenceId    156060 non-null int64\n",
      "Phrase        156060 non-null object\n",
      "Sentiment     156060 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "senti_known.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_known.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Five sentiment levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "Sentiment = set()\n",
    "for i in senti_known.Sentiment:\n",
    "    Sentiment.add(i)\n",
    "print(Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0 : negative\n",
    "- 1 : somewhat negative\n",
    "- 2 : neutral\n",
    "- 3 : somewhat positive\n",
    "- 4 : positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data set without sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 66292 entries, 0 to 66291\n",
      "Data columns (total 3 columns):\n",
      "PhraseId      66292 non-null int64\n",
      "SentenceId    66292 non-null int64\n",
      "Phrase        66292 non-null object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "senti_unknown.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_unknown.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id='C2'>3. Preprocessing of Text: Vectorizer</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Count Vectorizer with ngram_range=(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '000 leagues', '000 times', '10', '10 000', '10 15', '10 course', '10 minutes', '10 powerpuff', '10 seconds']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = senti_known['Sentiment']\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(senti_known.Phrase,y,test_size=0.33,random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer1 = CountVectorizer(stop_words= \"english\",ngram_range=(1,2))\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train1 = count_vectorizer1.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test1 = count_vectorizer1.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer1.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Count Vectorizer with ngram_range=(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000 leagues sea', '000 leagues sea george', '10 000 times', '10 15 minutes', '10 15 minutes cut', '10 15 minutes wendigo', '10 course banquet', '10 minutes film', '10 minutes film ll', '10 minutes past']\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer2 = CountVectorizer(stop_words= \"english\",ngram_range=(3,4))\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train2 = count_vectorizer2.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test2 = count_vectorizer2.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer2.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tfidf Vectorizer with ngram_range=(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000 leagues sea', '000 leagues sea george', '10 000 times', '10 15 minutes', '10 15 minutes cut', '10 15 minutes wendigo', '10 course banquet', '10 minutes film', '10 minutes film ll', '10 minutes past']\n",
      "  (0, 2148)\t0.710380191113\n",
      "  (0, 5437)\t0.461241602095\n",
      "  (0, 8522)\t0.531616561604\n",
      "  (1, 2559)\t0.590848100277\n",
      "  (1, 11239)\t0.635172612276\n",
      "  (1, 5571)\t0.49744776109\n",
      "  (2, 6550)\t0.690622630263\n",
      "  (2, 555)\t0.472487683103\n",
      "  (2, 13534)\t0.547536091856\n",
      "  (3, 8819)\t0.83865106412\n",
      "  (3, 11480)\t0.544669067095\n",
      "  (4, 10196)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer1 = TfidfVectorizer(stop_words='english',max_df=0.7,ngram_range=(3,4))\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train1 = tfidf_vectorizer1.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test1 = tfidf_vectorizer1.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer1.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tfidf Vectorizer with ngram_range=(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000 leagues sea', '000 leagues sea george', '10 000 times', '10 15 minutes', '10 15 minutes cut', '10 15 minutes wendigo', '10 course banquet', '10 minutes film', '10 minutes film ll', '10 minutes past']\n",
      "  (0, 2148)\t0.710380191113\n",
      "  (0, 5437)\t0.461241602095\n",
      "  (0, 8522)\t0.531616561604\n",
      "  (1, 2559)\t0.590848100277\n",
      "  (1, 11239)\t0.635172612276\n",
      "  (1, 5571)\t0.49744776109\n",
      "  (2, 6550)\t0.690622630263\n",
      "  (2, 555)\t0.472487683103\n",
      "  (2, 13534)\t0.547536091856\n",
      "  (3, 8819)\t0.83865106412\n",
      "  (3, 11480)\t0.544669067095\n",
      "  (4, 10196)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer2 = TfidfVectorizer(stop_words='english',max_df=0.7,ngram_range=(3,4))\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train2 = tfidf_vectorizer2.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test2 = tfidf_vectorizer2.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer2.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id='C4'>4. Modeling, Evaluation and Improving the Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train and evaluate the multinomial naive bayes model by the training data with count vectorier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ..., 2 3 2]\n",
      "0.60786407767\n",
      "[[  887  1054   325    45     4]\n",
      " [  877  4346  3439   426    44]\n",
      " [  352  2908 19176  3389   358]\n",
      " [   54   450  3522  5630  1125]\n",
      " [    4    42   320  1457  1266]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train1,y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test1)\n",
    "print(pred)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm =  metrics.confusion_matrix(y_test,pred,labels=[0,1,2,3,4])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Improving the model of 4.1 by tweaking alpha level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_range=(1,2)\n",
      "Alpha:  0.0\n",
      "Score:  0.560796116505\n",
      "Alpha:  0.1\n",
      "Score:  0.569126213592\n",
      "Alpha:  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.575708737864\n",
      "Alpha:  0.3\n",
      "Score:  0.58227184466\n",
      "Alpha:  0.4\n",
      "Score:  0.587359223301\n",
      "Alpha:  0.5\n",
      "Score:  0.592174757282\n",
      "Alpha:  0.6\n",
      "Score:  0.595747572816\n",
      "Alpha:  0.7\n",
      "Score:  0.599300970874\n",
      "Alpha:  0.8\n",
      "Score:  0.602757281553\n",
      "Alpha:  0.9\n",
      "Score:  0.605029126214\n",
      "Alpha:  1.0\n",
      "Score:  0.60786407767\n",
      "Alpha:  1.1\n",
      "Score:  0.609417475728\n",
      "Alpha:  1.2\n",
      "Score:  0.611009708738\n",
      "Alpha:  1.3\n",
      "Score:  0.61267961165\n",
      "Alpha:  1.4\n",
      "Score:  0.61454368932\n",
      "Alpha:  1.5\n",
      "Score:  0.614951456311\n",
      "Alpha:  1.6\n",
      "Score:  0.61627184466\n",
      "Alpha:  1.7\n",
      "Score:  0.617126213592\n",
      "Alpha:  1.8\n",
      "Score:  0.617825242718\n",
      "Alpha:  1.9\n",
      "Score:  0.618077669903\n",
      "Alpha:  2.0\n",
      "Score:  0.618116504854\n",
      "Alpha:  2.1\n",
      "Score:  0.619689320388\n",
      "Alpha:  2.2\n",
      "Score:  0.619417475728\n",
      "Alpha:  2.3\n",
      "Score:  0.619126213592\n",
      "Alpha:  2.4\n",
      "Score:  0.619902912621\n",
      "Alpha:  2.5\n",
      "Score:  0.619766990291\n",
      "Alpha:  2.6\n",
      "Score:  0.61959223301\n",
      "Alpha:  2.7\n",
      "Score:  0.619398058252\n",
      "Alpha:  2.8\n",
      "Score:  0.618990291262\n",
      "Alpha:  2.9\n",
      "Score:  0.619650485437\n",
      "Alpha:  3.0\n",
      "Score:  0.620058252427\n",
      "Alpha:  3.1\n",
      "Score:  0.619825242718\n",
      "Alpha:  3.2\n",
      "Score:  0.619553398058\n",
      "Alpha:  3.3\n",
      "Score:  0.619106796117\n",
      "Alpha:  3.4\n",
      "Score:  0.619165048544\n",
      "Alpha:  3.5\n",
      "Score:  0.619067961165\n",
      "Alpha:  3.6\n",
      "Score:  0.618951456311\n",
      "Alpha:  3.7\n",
      "Score:  0.618485436893\n",
      "Alpha:  3.8\n",
      "Score:  0.618116504854\n",
      "Alpha:  3.9\n",
      "Score:  0.617708737864\n",
      "Alpha:  4.0\n",
      "Score:  0.617495145631\n",
      "Alpha:  4.1\n",
      "Score:  0.617165048544\n",
      "Alpha:  4.2\n",
      "Score:  0.616970873786\n",
      "Alpha:  4.3\n",
      "Score:  0.616893203883\n",
      "Alpha:  4.4\n",
      "Score:  0.616466019417\n",
      "Alpha:  4.5\n",
      "Score:  0.616737864078\n",
      "Alpha:  4.6\n",
      "Score:  0.616174757282\n",
      "Alpha:  4.7\n",
      "Score:  0.615378640777\n",
      "Alpha:  4.8\n",
      "Score:  0.614815533981\n",
      "Alpha:  4.9\n",
      "Score:  0.614330097087\n",
      "ngram_range=(3,4)\n",
      "Alpha:  0.0\n",
      "Score:  0.575961165049\n",
      "Alpha:  0.1\n",
      "Score:  0.578776699029\n",
      "Alpha:  0.2\n",
      "Score:  0.580834951456\n",
      "Alpha:  0.3\n",
      "Score:  0.581747572816\n",
      "Alpha:  0.4\n",
      "Score:  0.582621359223\n",
      "Alpha:  0.5\n",
      "Score:  0.582951456311\n",
      "Alpha:  0.6\n",
      "Score:  0.583708737864\n",
      "Alpha:  0.7\n",
      "Score:  0.583262135922\n",
      "Alpha:  0.8\n",
      "Score:  0.581708737864\n",
      "Alpha:  0.9\n",
      "Score:  0.581689320388\n",
      "Alpha:  1.0\n",
      "Score:  0.581922330097\n",
      "Alpha:  1.1\n",
      "Score:  0.582155339806\n",
      "Alpha:  1.2\n",
      "Score:  0.582252427184\n",
      "Alpha:  1.3\n",
      "Score:  0.582660194175\n",
      "Alpha:  1.4\n",
      "Score:  0.583300970874\n",
      "Alpha:  1.5\n",
      "Score:  0.580291262136\n",
      "Alpha:  1.6\n",
      "Score:  0.580368932039\n",
      "Alpha:  1.7\n",
      "Score:  0.580368932039\n",
      "Alpha:  1.8\n",
      "Score:  0.579883495146\n",
      "Alpha:  1.9\n",
      "Score:  0.579747572816\n",
      "Alpha:  2.0\n",
      "Score:  0.580038834951\n",
      "Alpha:  2.1\n",
      "Score:  0.579631067961\n",
      "Alpha:  2.2\n",
      "Score:  0.577922330097\n",
      "Alpha:  2.3\n",
      "Score:  0.57772815534\n",
      "Alpha:  2.4\n",
      "Score:  0.578\n",
      "Alpha:  2.5\n",
      "Score:  0.577786407767\n",
      "Alpha:  2.6\n",
      "Score:  0.577650485437\n",
      "Alpha:  2.7\n",
      "Score:  0.577242718447\n",
      "Alpha:  2.8\n",
      "Score:  0.57667961165\n",
      "Alpha:  2.9\n",
      "Score:  0.575126213592\n",
      "Alpha:  3.0\n",
      "Score:  0.575184466019\n",
      "Alpha:  3.1\n",
      "Score:  0.574194174757\n",
      "Alpha:  3.2\n",
      "Score:  0.574077669903\n",
      "Alpha:  3.3\n",
      "Score:  0.573708737864\n",
      "Alpha:  3.4\n",
      "Score:  0.57359223301\n",
      "Alpha:  3.5\n",
      "Score:  0.573184466019\n",
      "Alpha:  3.6\n",
      "Score:  0.573184466019\n",
      "Alpha:  3.7\n",
      "Score:  0.572524271845\n",
      "Alpha:  3.8\n",
      "Score:  0.572349514563\n",
      "Alpha:  3.9\n",
      "Score:  0.57213592233\n",
      "Alpha:  4.0\n",
      "Score:  0.57186407767\n",
      "Alpha:  4.1\n",
      "Score:  0.570854368932\n",
      "Alpha:  4.2\n",
      "Score:  0.570718446602\n",
      "Alpha:  4.3\n",
      "Score:  0.570368932039\n",
      "Alpha:  4.4\n",
      "Score:  0.570155339806\n",
      "Alpha:  4.5\n",
      "Score:  0.570097087379\n",
      "Alpha:  4.6\n",
      "Score:  0.57013592233\n",
      "Alpha:  4.7\n",
      "Score:  0.57013592233\n",
      "Alpha:  4.8\n",
      "Score:  0.569922330097\n",
      "Alpha:  4.9\n",
      "Score:  0.569436893204\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,5,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha,count_train,count_test):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(count_train,y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(count_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "print('ngram_range=(1,2)')\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha,count_train1,count_test1))\n",
    "\n",
    "print('ngram_range=(3,4)')\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha,count_train2,count_test2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the outcome above, best alpha of multinomial NB is 2.4 for countvectorizer and (1,2) is a better ngram_range, and best score is 0.620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train and evaluate the multinomial naive bayes model by the training data with tfidf vectorier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.571339805825\n",
      "[[  142   600  1564     9     0]\n",
      " [   84  1628  7337    82     1]\n",
      " [   14   552 24901   707     9]\n",
      " [    0    84  8092  2493   112]\n",
      " [    0    11  1869   949   260]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train1,y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test1)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test,pred,labels=[0,1,2,3,4])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Improving the model of 4.3 by tweaking alpha level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_range=(1,2)\n",
      "Alpha:  0.0\n",
      "Score:  0.591223300971\n",
      "\n",
      "Alpha:  0.02\n",
      "Score:  0.593106796117\n",
      "\n",
      "Alpha:  0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.594388349515\n",
      "\n",
      "Alpha:  0.06\n",
      "Score:  0.595262135922\n",
      "\n",
      "Alpha:  0.08\n",
      "Score:  0.595844660194\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.596485436893\n",
      "\n",
      "Alpha:  0.12\n",
      "Score:  0.597126213592\n",
      "\n",
      "Alpha:  0.14\n",
      "Score:  0.597398058252\n",
      "\n",
      "Alpha:  0.16\n",
      "Score:  0.597766990291\n",
      "\n",
      "Alpha:  0.18\n",
      "Score:  0.598077669903\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.598291262136\n",
      "\n",
      "Alpha:  0.22\n",
      "Score:  0.598155339806\n",
      "\n",
      "Alpha:  0.24\n",
      "Score:  0.59827184466\n",
      "\n",
      "Alpha:  0.26\n",
      "Score:  0.597669902913\n",
      "\n",
      "Alpha:  0.28\n",
      "Score:  0.597631067961\n",
      "\n",
      "Alpha:  0.3\n",
      "Score:  0.597106796117\n",
      "\n",
      "Alpha:  0.32\n",
      "Score:  0.59640776699\n",
      "\n",
      "Alpha:  0.34\n",
      "Score:  0.595961165049\n",
      "\n",
      "Alpha:  0.36\n",
      "Score:  0.595514563107\n",
      "\n",
      "Alpha:  0.38\n",
      "Score:  0.595378640777\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.594834951456\n",
      "\n",
      "Alpha:  0.42\n",
      "Score:  0.593961165049\n",
      "\n",
      "Alpha:  0.44\n",
      "Score:  0.593281553398\n",
      "\n",
      "Alpha:  0.46\n",
      "Score:  0.592757281553\n",
      "\n",
      "Alpha:  0.48\n",
      "Score:  0.592058252427\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.591533980583\n",
      "\n",
      "Alpha:  0.52\n",
      "Score:  0.591378640777\n",
      "\n",
      "Alpha:  0.54\n",
      "Score:  0.590446601942\n",
      "\n",
      "Alpha:  0.56\n",
      "Score:  0.59\n",
      "\n",
      "Alpha:  0.58\n",
      "Score:  0.589339805825\n",
      "\n",
      "Alpha:  0.6\n",
      "Score:  0.588621359223\n",
      "\n",
      "Alpha:  0.62\n",
      "Score:  0.587087378641\n",
      "\n",
      "Alpha:  0.64\n",
      "Score:  0.586446601942\n",
      "\n",
      "Alpha:  0.66\n",
      "Score:  0.586\n",
      "\n",
      "Alpha:  0.68\n",
      "Score:  0.585339805825\n",
      "\n",
      "Alpha:  0.7\n",
      "Score:  0.584097087379\n",
      "\n",
      "Alpha:  0.72\n",
      "Score:  0.582854368932\n",
      "\n",
      "Alpha:  0.74\n",
      "Score:  0.582174757282\n",
      "\n",
      "Alpha:  0.76\n",
      "Score:  0.581825242718\n",
      "\n",
      "Alpha:  0.78\n",
      "Score:  0.581145631068\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.578310679612\n",
      "\n",
      "Alpha:  0.82\n",
      "Score:  0.577650485437\n",
      "\n",
      "Alpha:  0.84\n",
      "Score:  0.576660194175\n",
      "\n",
      "Alpha:  0.86\n",
      "Score:  0.575631067961\n",
      "\n",
      "Alpha:  0.88\n",
      "Score:  0.574854368932\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.574504854369\n",
      "\n",
      "Alpha:  0.92\n",
      "Score:  0.574310679612\n",
      "\n",
      "Alpha:  0.94\n",
      "Score:  0.57359223301\n",
      "\n",
      "Alpha:  0.96\n",
      "Score:  0.572737864078\n",
      "\n",
      "Alpha:  0.98\n",
      "Score:  0.572097087379\n",
      "\n",
      "ngram_range=(3,4)\n",
      "Alpha:  0.0\n",
      "Score:  0.591223300971\n",
      "\n",
      "Alpha:  0.02\n",
      "Score:  0.593106796117\n",
      "\n",
      "Alpha:  0.04\n",
      "Score:  0.594388349515\n",
      "\n",
      "Alpha:  0.06\n",
      "Score:  0.595262135922\n",
      "\n",
      "Alpha:  0.08\n",
      "Score:  0.595844660194\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.596485436893\n",
      "\n",
      "Alpha:  0.12\n",
      "Score:  0.597126213592\n",
      "\n",
      "Alpha:  0.14\n",
      "Score:  0.597398058252\n",
      "\n",
      "Alpha:  0.16\n",
      "Score:  0.597766990291\n",
      "\n",
      "Alpha:  0.18\n",
      "Score:  0.598077669903\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.598291262136\n",
      "\n",
      "Alpha:  0.22\n",
      "Score:  0.598155339806\n",
      "\n",
      "Alpha:  0.24\n",
      "Score:  0.59827184466\n",
      "\n",
      "Alpha:  0.26\n",
      "Score:  0.597669902913\n",
      "\n",
      "Alpha:  0.28\n",
      "Score:  0.597631067961\n",
      "\n",
      "Alpha:  0.3\n",
      "Score:  0.597106796117\n",
      "\n",
      "Alpha:  0.32\n",
      "Score:  0.59640776699\n",
      "\n",
      "Alpha:  0.34\n",
      "Score:  0.595961165049\n",
      "\n",
      "Alpha:  0.36\n",
      "Score:  0.595514563107\n",
      "\n",
      "Alpha:  0.38\n",
      "Score:  0.595378640777\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.594834951456\n",
      "\n",
      "Alpha:  0.42\n",
      "Score:  0.593961165049\n",
      "\n",
      "Alpha:  0.44\n",
      "Score:  0.593281553398\n",
      "\n",
      "Alpha:  0.46\n",
      "Score:  0.592757281553\n",
      "\n",
      "Alpha:  0.48\n",
      "Score:  0.592058252427\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.591533980583\n",
      "\n",
      "Alpha:  0.52\n",
      "Score:  0.591378640777\n",
      "\n",
      "Alpha:  0.54\n",
      "Score:  0.590446601942\n",
      "\n",
      "Alpha:  0.56\n",
      "Score:  0.59\n",
      "\n",
      "Alpha:  0.58\n",
      "Score:  0.589339805825\n",
      "\n",
      "Alpha:  0.6\n",
      "Score:  0.588621359223\n",
      "\n",
      "Alpha:  0.62\n",
      "Score:  0.587087378641\n",
      "\n",
      "Alpha:  0.64\n",
      "Score:  0.586446601942\n",
      "\n",
      "Alpha:  0.66\n",
      "Score:  0.586\n",
      "\n",
      "Alpha:  0.68\n",
      "Score:  0.585339805825\n",
      "\n",
      "Alpha:  0.7\n",
      "Score:  0.584097087379\n",
      "\n",
      "Alpha:  0.72\n",
      "Score:  0.582854368932\n",
      "\n",
      "Alpha:  0.74\n",
      "Score:  0.582174757282\n",
      "\n",
      "Alpha:  0.76\n",
      "Score:  0.581825242718\n",
      "\n",
      "Alpha:  0.78\n",
      "Score:  0.581145631068\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.578310679612\n",
      "\n",
      "Alpha:  0.82\n",
      "Score:  0.577650485437\n",
      "\n",
      "Alpha:  0.84\n",
      "Score:  0.576660194175\n",
      "\n",
      "Alpha:  0.86\n",
      "Score:  0.575631067961\n",
      "\n",
      "Alpha:  0.88\n",
      "Score:  0.574854368932\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.574504854369\n",
      "\n",
      "Alpha:  0.92\n",
      "Score:  0.574310679612\n",
      "\n",
      "Alpha:  0.94\n",
      "Score:  0.57359223301\n",
      "\n",
      "Alpha:  0.96\n",
      "Score:  0.572737864078\n",
      "\n",
      "Alpha:  0.98\n",
      "Score:  0.572097087379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.02)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha,tfidf_train,tfidf_test):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train,y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "print('ngram_range=(1,2)')\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha,tfidf_train1,tfidf_test1))\n",
    "    print()\n",
    "\n",
    "print('ngram_range=(3,4)')\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha,tfidf_train2,tfidf_test2))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the outcome above, best alpha of multinomial NB is 0.2 for tfidfvectorizer and (1,2) is a better ngram_range, and best score is 0.598."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5  Inspect the model and explore the vector weights of actual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [(-11.603903263989569, '000'), (-11.603903263989569, '000 leagues'), (-11.603903263989569, '10 000'), (-11.603903263989569, '10 15'), (-11.603903263989569, '10 course'), (-11.603903263989569, '10 minutes'), (-11.603903263989569, '10 powerpuff'), (-11.603903263989569, '10 unlikely'), (-11.603903263989569, '10 year'), (-11.603903263989569, '100 minute'), (-11.603903263989569, '100 minutes'), (-11.603903263989569, '100 year'), (-11.603903263989569, '100 years'), (-11.603903263989569, '101'), (-11.603903263989569, '101 minutes'), (-11.603903263989569, '101 poetic'), (-11.603903263989569, '101 premise'), (-11.603903263989569, '102'), (-11.603903263989569, '102 minute'), (-11.603903263989569, '103 minute')]\n",
      "1 [(-10.118002703003793, 'movies life'), (-10.110902491805263, 'stylistically borrows'), (-10.098716949406356, 'theater shout'), (-10.091010963144793, 'stanford'), (-10.068856652280468, 'cox offers'), (-10.039781025632665, 'clerk wanting'), (-10.035493616140164, 'norrington'), (-10.029903471277335, 'adrenaline documentary'), (-10.029903471277335, 'far bigger'), (-10.029903471277335, 'far bow'), (-10.015971371952599, 'flat script'), (-9.9944653515554691, 'endearing hero'), (-9.9520583798952504, 'body'), (-9.9457612675437268, 'chou'), (-9.9317615819239276, 'schoolgirl obsession'), (-9.8627420660183756, 'burns best'), (-9.8121437947615142, 'like 10'), (-9.8121437947615142, 'pass litmus'), (-9.7268051308840953, 'illuminating study'), (-9.6840670680555654, 'schoolgirl')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = count_vectorizer1.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id='C5'> 5. Predict the Sentiment labels of Sentiment_unknow Set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 2 ..., 2 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11b597bf278>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b58fea860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_senti_unknown = senti_unknown.Phrase\n",
    "count_senti_unknown = count_vectorizer1.transform(X_senti_unknown)\n",
    "# Instantiate the classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB(alpha=2.4)\n",
    "# Fit to the training data\n",
    "nb_classifier.fit(count_train1,y_train)\n",
    "pred_senti_unknown = nb_classifier.predict(count_senti_unknown)\n",
    "print(pred_senti_unknown)\n",
    "_ = plt.hist(pred_senti_unknown)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id='C6'>6. Conclusion</h2>\n",
    "<p>In this project, a mutilnomial model is built to analyze the sentiment of moie reviews. Compared with tfidf vectoriser, count vectoriser is a better choice. And best alpha is 2.4 for the model trained by data with count vectorizer(ngram_range=(1,2)), while it's 0.02 for tfidf vectoriser. And best accuracy score with this model is 0.620. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</html>\n",
    "</body>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
