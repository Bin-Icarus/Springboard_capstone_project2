{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Description:\n",
    "\n",
    "<p>\n",
    "Opinion mining or sentiment analysis aims to determine the attitude of a critic or customer or other subject with respected to a topic. And in this project the goal is to figure out the attitude of critics towards various movies, predict the result of test set, and evaluate the model.\n",
    "</p>\n",
    "<p>Cognitive insight is important for online streaming or shopping website, because it can help to predict whether customers like or dislike a movie or other products. When they submit a new comment, the website can offer some feedback according to customersâ€™ attitude. And in this project, the data is from kaggle.com.\n",
    "</p>\n",
    "<p>And the goal of the project is to produce an algorithm to classify phrases into 5 categories: negative, somewhat negative, neutral, somewaht postive and positive.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "senti_known= pd.read_csv('train.tsv/train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "senti_unknown = pd.read_csv('test.tsv/test.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset and Have a First Look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data set with sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      "PhraseId      156060 non-null int64\n",
      "SentenceId    156060 non-null int64\n",
      "Phrase        156060 non-null object\n",
      "Sentiment     156060 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "senti_known.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_known.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Five sentiment levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "Sentiment = set()\n",
    "for i in senti_known.Sentiment:\n",
    "    Sentiment.add(i)\n",
    "print(Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0 : negative\n",
    "- 1 : somewhat negative\n",
    "- 2 : neutral\n",
    "- 3 : somewhat positive\n",
    "- 4 : positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data set without sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 66292 entries, 0 to 66291\n",
      "Data columns (total 3 columns):\n",
      "PhraseId      66292 non-null int64\n",
      "SentenceId    66292 non-null int64\n",
      "Phrase        66292 non-null object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "senti_unknown.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_unknown.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing of Text: Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '100', '101', '102', '103', '104', '105', '10th', '11']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = senti_known['Sentiment']\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(senti_known.Phrase,y,test_size=0.33,random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words= \"english\")\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '100', '101', '102', '103', '104', '105', '10th', '11']\n",
      "  (0, 2148)\t0.710380191113\n",
      "  (0, 5437)\t0.461241602095\n",
      "  (0, 8522)\t0.531616561604\n",
      "  (1, 2559)\t0.590848100277\n",
      "  (1, 11239)\t0.635172612276\n",
      "  (1, 5571)\t0.49744776109\n",
      "  (2, 6550)\t0.690622630263\n",
      "  (2, 555)\t0.472487683103\n",
      "  (2, 13534)\t0.547536091856\n",
      "  (3, 8819)\t0.83865106412\n",
      "  (3, 11480)\t0.544669067095\n",
      "  (4, 10196)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling, Evaluation and Improving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train and evaluate the multinomial naive bayes model by the training data with count vectorier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ..., 2 3 2]\n",
      "0.603553398058\n",
      "[[  635  1063   532    81     4]\n",
      " [  581  3720  4271   531    29]\n",
      " [  221  2246 20577  2939   200]\n",
      " [   34   400  4344  5347   656]\n",
      " [    2    40   535  1708   804]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train,y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "print(pred)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm =  metrics.confusion_matrix(y_test,pred,labels=[0,1,2,3,4])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Improving the model of 4.1 by tweaking alpha level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.592834951456\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.595184466019\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.596563106796\n",
      "\n",
      "Alpha:  0.3\n",
      "Score:  0.598019417476\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.598932038835\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.599902912621\n",
      "\n",
      "Alpha:  0.6\n",
      "Score:  0.601805825243\n",
      "\n",
      "Alpha:  0.7\n",
      "Score:  0.602582524272\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.603048543689\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.603398058252\n",
      "\n",
      "Alpha:  1.0\n",
      "Score:  0.603553398058\n",
      "\n",
      "Alpha:  1.1\n",
      "Score:  0.604213592233\n",
      "\n",
      "Alpha:  1.2\n",
      "Score:  0.603844660194\n",
      "\n",
      "Alpha:  1.3\n",
      "Score:  0.603941747573\n",
      "\n",
      "Alpha:  1.4\n",
      "Score:  0.604213592233\n",
      "\n",
      "Alpha:  1.5\n",
      "Score:  0.605495145631\n",
      "\n",
      "Alpha:  1.6\n",
      "Score:  0.606058252427\n",
      "\n",
      "Alpha:  1.7\n",
      "Score:  0.606291262136\n",
      "\n",
      "Alpha:  1.8\n",
      "Score:  0.606621359223\n",
      "\n",
      "Alpha:  1.9\n",
      "Score:  0.607184466019\n",
      "\n",
      "Alpha:  2.0\n",
      "Score:  0.607281553398\n",
      "\n",
      "Alpha:  2.1\n",
      "Score:  0.606873786408\n",
      "\n",
      "Alpha:  2.2\n",
      "Score:  0.60640776699\n",
      "\n",
      "Alpha:  2.3\n",
      "Score:  0.605902912621\n",
      "\n",
      "Alpha:  2.4\n",
      "Score:  0.606252427184\n",
      "\n",
      "Alpha:  2.5\n",
      "Score:  0.606077669903\n",
      "\n",
      "Alpha:  2.6\n",
      "Score:  0.605844660194\n",
      "\n",
      "Alpha:  2.7\n",
      "Score:  0.605980582524\n",
      "\n",
      "Alpha:  2.8\n",
      "Score:  0.605572815534\n",
      "\n",
      "Alpha:  2.9\n",
      "Score:  0.605359223301\n",
      "\n",
      "Alpha:  3.0\n",
      "Score:  0.605029126214\n",
      "\n",
      "Alpha:  3.1\n",
      "Score:  0.604718446602\n",
      "\n",
      "Alpha:  3.2\n",
      "Score:  0.605126213592\n",
      "\n",
      "Alpha:  3.3\n",
      "Score:  0.604970873786\n",
      "\n",
      "Alpha:  3.4\n",
      "Score:  0.604388349515\n",
      "\n",
      "Alpha:  3.5\n",
      "Score:  0.604601941748\n",
      "\n",
      "Alpha:  3.6\n",
      "Score:  0.603941747573\n",
      "\n",
      "Alpha:  3.7\n",
      "Score:  0.604038834951\n",
      "\n",
      "Alpha:  3.8\n",
      "Score:  0.603436893204\n",
      "\n",
      "Alpha:  3.9\n",
      "Score:  0.603203883495\n",
      "\n",
      "Alpha:  4.0\n",
      "Score:  0.60254368932\n",
      "\n",
      "Alpha:  4.1\n",
      "Score:  0.602233009709\n",
      "\n",
      "Alpha:  4.2\n",
      "Score:  0.601650485437\n",
      "\n",
      "Alpha:  4.3\n",
      "Score:  0.601029126214\n",
      "\n",
      "Alpha:  4.4\n",
      "Score:  0.600737864078\n",
      "\n",
      "Alpha:  4.5\n",
      "Score:  0.600446601942\n",
      "\n",
      "Alpha:  4.6\n",
      "Score:  0.59972815534\n",
      "\n",
      "Alpha:  4.7\n",
      "Score:  0.599436893204\n",
      "\n",
      "Alpha:  4.8\n",
      "Score:  0.598640776699\n",
      "\n",
      "Alpha:  4.9\n",
      "Score:  0.598349514563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,5,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(count_train,y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(count_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train and evaluate the multinomial naive bayes model by the training data with tfidf vectorier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584640776699\n",
      "[[   96   940  1231    48     0]\n",
      " [   47  2458  6322   301     4]\n",
      " [   19  1151 23251  1740    22]\n",
      " [    3   136  6408  4166    68]\n",
      " [    0    11  1125  1815   138]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train,y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test,pred,labels=[0,1,2,3,4])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Improving the model of 4.3 by tweaking alpha level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.592893203883\n",
      "\n",
      "Alpha:  0.01\n",
      "Score:  0.592951456311\n",
      "\n",
      "Alpha:  0.02\n",
      "Score:  0.592990291262\n",
      "\n",
      "Alpha:  0.03\n",
      "Score:  0.592718446602\n",
      "\n",
      "Alpha:  0.04\n",
      "Score:  0.592796116505\n",
      "\n",
      "Alpha:  0.05\n",
      "Score:  0.592932038835\n",
      "\n",
      "Alpha:  0.06\n",
      "Score:  0.592796116505\n",
      "\n",
      "Alpha:  0.07\n",
      "Score:  0.59254368932\n",
      "\n",
      "Alpha:  0.08\n",
      "Score:  0.592504854369\n",
      "\n",
      "Alpha:  0.09\n",
      "Score:  0.592427184466\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.592660194175\n",
      "\n",
      "Alpha:  0.11\n",
      "Score:  0.59254368932\n",
      "\n",
      "Alpha:  0.12\n",
      "Score:  0.59254368932\n",
      "\n",
      "Alpha:  0.13\n",
      "Score:  0.592466019417\n",
      "\n",
      "Alpha:  0.14\n",
      "Score:  0.592330097087\n",
      "\n",
      "Alpha:  0.15\n",
      "Score:  0.591941747573\n",
      "\n",
      "Alpha:  0.16\n",
      "Score:  0.591631067961\n",
      "\n",
      "Alpha:  0.17\n",
      "Score:  0.591572815534\n",
      "\n",
      "Alpha:  0.18\n",
      "Score:  0.591669902913\n",
      "\n",
      "Alpha:  0.19\n",
      "Score:  0.591495145631\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.591631067961\n",
      "\n",
      "Alpha:  0.21\n",
      "Score:  0.591417475728\n",
      "\n",
      "Alpha:  0.22\n",
      "Score:  0.591339805825\n",
      "\n",
      "Alpha:  0.23\n",
      "Score:  0.591126213592\n",
      "\n",
      "Alpha:  0.24\n",
      "Score:  0.591029126214\n",
      "\n",
      "Alpha:  0.25\n",
      "Score:  0.590893203883\n",
      "\n",
      "Alpha:  0.26\n",
      "Score:  0.590796116505\n",
      "\n",
      "Alpha:  0.27\n",
      "Score:  0.590504854369\n",
      "\n",
      "Alpha:  0.28\n",
      "Score:  0.590563106796\n",
      "\n",
      "Alpha:  0.29\n",
      "Score:  0.590563106796\n",
      "\n",
      "Alpha:  0.3\n",
      "Score:  0.59040776699\n",
      "\n",
      "Alpha:  0.31\n",
      "Score:  0.590213592233\n",
      "\n",
      "Alpha:  0.32\n",
      "Score:  0.590155339806\n",
      "\n",
      "Alpha:  0.33\n",
      "Score:  0.589747572816\n",
      "\n",
      "Alpha:  0.34\n",
      "Score:  0.589844660194\n",
      "\n",
      "Alpha:  0.35\n",
      "Score:  0.589883495146\n",
      "\n",
      "Alpha:  0.36\n",
      "Score:  0.589631067961\n",
      "\n",
      "Alpha:  0.37\n",
      "Score:  0.589495145631\n",
      "\n",
      "Alpha:  0.38\n",
      "Score:  0.589339805825\n",
      "\n",
      "Alpha:  0.39\n",
      "Score:  0.589339805825\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.589398058252\n",
      "\n",
      "Alpha:  0.41\n",
      "Score:  0.589281553398\n",
      "\n",
      "Alpha:  0.42\n",
      "Score:  0.589378640777\n",
      "\n",
      "Alpha:  0.43\n",
      "Score:  0.589145631068\n",
      "\n",
      "Alpha:  0.44\n",
      "Score:  0.588776699029\n",
      "\n",
      "Alpha:  0.45\n",
      "Score:  0.588854368932\n",
      "\n",
      "Alpha:  0.46\n",
      "Score:  0.588737864078\n",
      "\n",
      "Alpha:  0.47\n",
      "Score:  0.588796116505\n",
      "\n",
      "Alpha:  0.48\n",
      "Score:  0.588796116505\n",
      "\n",
      "Alpha:  0.49\n",
      "Score:  0.58867961165\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.588446601942\n",
      "\n",
      "Alpha:  0.51\n",
      "Score:  0.588368932039\n",
      "\n",
      "Alpha:  0.52\n",
      "Score:  0.588291262136\n",
      "\n",
      "Alpha:  0.53\n",
      "Score:  0.588291262136\n",
      "\n",
      "Alpha:  0.54\n",
      "Score:  0.58813592233\n",
      "\n",
      "Alpha:  0.55\n",
      "Score:  0.588019417476\n",
      "\n",
      "Alpha:  0.56\n",
      "Score:  0.587844660194\n",
      "\n",
      "Alpha:  0.57\n",
      "Score:  0.58759223301\n",
      "\n",
      "Alpha:  0.58\n",
      "Score:  0.58772815534\n",
      "\n",
      "Alpha:  0.59\n",
      "Score:  0.587747572816\n",
      "\n",
      "Alpha:  0.6\n",
      "Score:  0.587533980583\n",
      "\n",
      "Alpha:  0.61\n",
      "Score:  0.587572815534\n",
      "\n",
      "Alpha:  0.62\n",
      "Score:  0.587495145631\n",
      "\n",
      "Alpha:  0.63\n",
      "Score:  0.587398058252\n",
      "\n",
      "Alpha:  0.64\n",
      "Score:  0.587262135922\n",
      "\n",
      "Alpha:  0.65\n",
      "Score:  0.586970873786\n",
      "\n",
      "Alpha:  0.66\n",
      "Score:  0.587029126214\n",
      "\n",
      "Alpha:  0.67\n",
      "Score:  0.587165048544\n",
      "\n",
      "Alpha:  0.68\n",
      "Score:  0.587048543689\n",
      "\n",
      "Alpha:  0.69\n",
      "Score:  0.586970873786\n",
      "\n",
      "Alpha:  0.7\n",
      "Score:  0.586970873786\n",
      "\n",
      "Alpha:  0.71\n",
      "Score:  0.586990291262\n",
      "\n",
      "Alpha:  0.72\n",
      "Score:  0.586893203883\n",
      "\n",
      "Alpha:  0.73\n",
      "Score:  0.586563106796\n",
      "\n",
      "Alpha:  0.74\n",
      "Score:  0.58654368932\n",
      "\n",
      "Alpha:  0.75\n",
      "Score:  0.586466019417\n",
      "\n",
      "Alpha:  0.76\n",
      "Score:  0.586330097087\n",
      "\n",
      "Alpha:  0.77\n",
      "Score:  0.58640776699\n",
      "\n",
      "Alpha:  0.78\n",
      "Score:  0.586194174757\n",
      "\n",
      "Alpha:  0.79\n",
      "Score:  0.586155339806\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.586038834951\n",
      "\n",
      "Alpha:  0.81\n",
      "Score:  0.585941747573\n",
      "\n",
      "Alpha:  0.82\n",
      "Score:  0.585961165049\n",
      "\n",
      "Alpha:  0.83\n",
      "Score:  0.585631067961\n",
      "\n",
      "Alpha:  0.84\n",
      "Score:  0.585378640777\n",
      "\n",
      "Alpha:  0.85\n",
      "Score:  0.585242718447\n",
      "\n",
      "Alpha:  0.86\n",
      "Score:  0.584932038835\n",
      "\n",
      "Alpha:  0.87\n",
      "Score:  0.584932038835\n",
      "\n",
      "Alpha:  0.88\n",
      "Score:  0.584854368932\n",
      "\n",
      "Alpha:  0.89\n",
      "Score:  0.584757281553\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.584718446602\n",
      "\n",
      "Alpha:  0.91\n",
      "Score:  0.584796116505\n",
      "\n",
      "Alpha:  0.92\n",
      "Score:  0.584776699029\n",
      "\n",
      "Alpha:  0.93\n",
      "Score:  0.584718446602\n",
      "\n",
      "Alpha:  0.94\n",
      "Score:  0.584893203883\n",
      "\n",
      "Alpha:  0.95\n",
      "Score:  0.584912621359\n",
      "\n",
      "Alpha:  0.96\n",
      "Score:  0.584932038835\n",
      "\n",
      "Alpha:  0.97\n",
      "Score:  0.584951456311\n",
      "\n",
      "Alpha:  0.98\n",
      "Score:  0.584912621359\n",
      "\n",
      "Alpha:  0.99\n",
      "Score:  0.584815533981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.01)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train,y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5  Inspect the model and explore the vector weights of actual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [(-10.268495183013016, '10th'), (-10.268495183013016, '127'), (-10.268495183013016, '129'), (-10.268495183013016, '12th'), (-10.268495183013016, '13th'), (-10.268495183013016, '14'), (-10.268495183013016, '15th'), (-10.268495183013016, '16'), (-10.268495183013016, '163'), (-10.268495183013016, '168'), (-10.268495183013016, '170'), (-10.268495183013016, '1790'), (-10.268495183013016, '18'), (-10.268495183013016, '1899'), (-10.268495183013016, '1915'), (-10.268495183013016, '1930s'), (-10.268495183013016, '1933'), (-10.268495183013016, '1937'), (-10.268495183013016, '1938'), (-10.268495183013016, '1940s')]\n",
      "1 [(-6.6309090232866303, 'dialogue'), (-6.5549231163087081, 'long'), (-6.5189911070826447, 'lrb'), (-6.5072950673194541, 'action'), (-6.5072950673194541, 'way'), (-6.4618326932426964, 'plot'), (-6.4618326932426964, 'rrb'), (-6.4507828570561117, 'dull'), (-6.4183475813029576, 'worst'), (-6.3766748849023891, 'does'), (-6.3564721775848696, 'story'), (-6.3076820134154383, 'minutes'), (-6.1909577391072963, 'characters'), (-6.1909577391072963, 'time'), (-6.1658518179762201, 'comedy'), (-6.1096120996533445, 'just'), (-5.5149049919066515, 'like'), (-5.2380572616205807, 'bad'), (-5.1747449822062537, 'film'), (-4.7135994890645323, 'movie')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict the Sentiment labels of Sentiment_unknow Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 2 ..., 2 2 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([   368.,      0.,   6545.,      0.,      0.,  46266.,      0.,\n",
       "         12303.,      0.,    810.]),\n",
       " array([ 0. ,  0.4,  0.8,  1.2,  1.6,  2. ,  2.4,  2.8,  3.2,  3.6,  4. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c5faf4be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_senti_unknown = senti_unknown.Phrase\n",
    "count_senti_unknown = count_vectorizer.transform(X_senti_unknown)\n",
    "# Instantiate the classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB(alpha=2.0)\n",
    "# Fit to the training data\n",
    "nb_classifier.fit(count_train,y_train)\n",
    "pred_senti_unknown = nb_classifier.predict(count_senti_unknown)\n",
    "print(pred_senti_unknown)\n",
    "plt.hist(pred_senti_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Conclusion:\n",
    "In this project, a mutilnomial model is built to analyze the sentiment of moie reviews. Compared with tfidf vectoriser, count vectoriser is a better choice. And best alpha is 2.0 for the model trained by data with count vectoriser, while it's 0.02 for tfidf vectoriser. And best accuracy score with this model is 6.07. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
